# 守护进程

`yadcc`大多数逻辑位于[守护进程](../daemon)。

取决于运行守护进程的用户，守护进程可能运行于启动账户或（如果由`root`启动会降低权限至）`daemon` / `nobody`账户。

守护进程目前实际上同时肩负两种职责：

- 处理本地请求：接收本地编译器wrapper（即[`client`](../client)）提交的任务、请求[调度器](../scheduler)获取空闲的编译机、将任务提交至具体编译机、等待任务完成并将结果返回给wrapper。
- 处理网络请求：接受网络上其他编译机提交的任务、执行、和提交方通信（返回结果等）。

后续我们将会针对两种职责分别进行介绍。

## 参数

- `--scheduler_uri`：[调度器](scheduler.md)的地址，通常形如`flare://ip-port-of-scheduler`或其他Flare能够解析的地址格式。

  目前调度器并不支持高可用，因此即便使用某些名字解析服务地址，解析结果也只能为一台机器。（但是使用这种地址有助于减轻机房裁撤导致的IP变化的运维困难）。

- `--cache_server_uri`：[缓存服务器](cache.md)地址，格式同`--scheduler-uri`。通常解析结果只能为一台服务器。

- `--token`：用于请求调度器、缓存服务器的`token`。具体能被调度器、缓存服务器接受的`token`列表取决于这两个服务器的配置（`--acceptable_tokens`）。

- `--temporary_dir`：临时文件存放路径。出于IO性能考虑，我们默认使用`/dev/shm`（“内存盘”）。对于部分环境如果`/dev/shm`容量很小（如几十M），可以通过这一参数改为其他路径，如`/tmp`。

- `--servant_priority`：可以指定为`user` / `dedicated`，不指定时默认`user`。如果配置为`dedicated`，则会向调度器报告这是一台“专用编译机”。调度器会认为专用编译机负载、性能稳定（没有外接负载）、并会优先将任务指派至专用编译机。（具体逻辑可参考[调度算法](scheduler.md)。）

- `--max_remote_tasks`：对于编译机而言，这个参数控制了最大的并发任务数。**默认值为不接受任务，对于编译机需要手动指定这一参数**。

- `--extra_compiler_dirs`：如果希望将不在`PATH`中的编译器也加入这一编译机所能提供的编译环境中，可以通过这一参数指定一系列`/path1/to/compiler/bin:/path2/to/compiler/bin`。

### 高级配置

下述参数通常不需要配置，但是对于特殊场景可能需要修改：

- `--allow_core_dump`：启用时打开Core dump，默认不启用。

- `--max_local_tasks`：本地并发任务数，真机环境默认为`核数 / 2`，容器环境为`cfs_quota_us / cfs_period_us / 2`。至少为1。

- `--lightweight_local_task_overprovisioning_ratio`：针对预处理等轻量任务，我们会额外允许一些任务并发度，这个“额外”的部分为`max_local_tasks * lightweight_local_task_overprovisioning_ratio`。

- `--min_memory_for_starting_new_task`：出于保护目的，对于编译机而言，如果发现本机内存已经不足这一配置，则会主动拒绝任务。但是需要注意的是，这并不是一个万全的措施。单个编译进程随着编译的进展，内存占用会逐渐上涨，这种情况下这一参数无法起到保护作用。

- `--poor_machine_threshold`：我们默认不会向配置较差的机器发送编译任务（`--servant_priority=dedicated`会忽略这一选项）。如果机器的CPU核心数小于或等于这一参数，则这台机器会认为是“配置较差”的。

## 处理本地请求

对于本地请求，守护进程目前主要提供如下能力：

- 控制并发度。

  通常而言，为了尽可能的利用本地CPU（预处理）、网络编译机（编译），需要指定一个较大的并发度（100或更大，取决于本地预处理能力）。显然，如果不加以控制，在包括但不限于如下场景中，可能会在本地启动大量的任务导致爆内存（目前包括但不限于[`distcc`](https://github.com/distcc)、[icecream](https://github.com/icecc/icecream)及上述工具的衍生品等均可能存在此问题）：

  - 链接等不可分布式的任务。在我们的测试中，部分链接任务可以消耗数GB内存，如果不加以控制很容易耗尽机器内存导致包括但不限于死机、SSH断开（通常是sshd被OOM killer杀掉）等问题。
  - 分发任务失败（网络堵塞等）导致的本地重试。如果网络波动导致大量任务本地重试，如果不加以限制，也容易耗尽内存导致问题。

  在实际应用场景中，我们还针对本地任务的计算量（大约估计）分成了如下两类任务：

  - 轻量任务。这一类任务主要对应于预处理，我们允许至多1.5 * CPU核数的并发度。这有助于覆盖IO导致的CPU空闲。这类任务通常CPU、内存开销均较低，因此过度供给CPU不存在实际操作上的问题。
  - 重量任务。非“轻量任务”均归于这一类。包括但不限于链接等操作。我们允许至多0.5 * CPU核数的并发度。一方面为了避免因为这种任务过多阻塞预处理等，一方面也避免消耗过多的内存导致问题。

- 处理编译任务。

  编译器wrapper本身并不直接请求网络。通过将任务提交给本地守护进程再由守护进程请求调度器、编译机等。

  这样的设计允许我们维护一定的状态，并允许我们以此改善性能。这包括但不限于：

  - 改善编译缓存访问效率：由于我们可以在守护进程中维护状态，我们实际上会维护一个（定期通过调度器同步的）[布隆过滤器](https://zh.wikipedia.org/zh-cn/%E5%B8%83%E9%9A%86%E8%BF%87%E6%BB%A4%E5%99%A8)来过滤掉不会命中缓存的请求。这可以节省我们一次网络RTT的延迟。

    这儿需要注意到我们的缓存实际的服务目标。除非构建系统自身存在问题、用户手动删除了所有的编译结果重新编译、或将代码修改后再改回来（偶尔存在这种场景），否则通常（如使用`blade build //path/to:target`等）会触发编译的目标均是自身或依赖发生了变化。因此**本地修改的编译通常不会命中缓存**。但是另一方面，如果编译的代码是新`git pull`到本地的，那么有可能在提交代码本地编译/测试（如`git leflow push`跑单测）时已经填充了缓存，这种情况下可以**复用代码提交人的编译缓存**。

    因此，不同于一般的缓存，对于编译场景，我们一方面需要尽可能的复用编译结果，另一方面也要避免无意义的缓存查询。

  - 复用网络连接：无论是请求调度的连接建立开销，又或是请求编译机的TCP慢启动爬升，通过将任务交给守护进程并复用已有的网络连接均可以节省多个网络RTT。

  - 改善请求调度器效率：这主要体现在如下几方面：

    - 编译配额预取：相对于每次编译时再请求调度器获取编译机，我们实际上会从调度器处预取1个编译任务的编译机。因此我们对于新到达的（预处理完的）编译任务，可以使用已经预取过的编译机的配额并直接提交，节省一次请求调度器的延迟。同时，我们会再发起一次新的异步预取，以满足下次编译。

      由于配额在没有Keep-Alive的情况下会自动过期，并且预取只会在当前存在编译任务的情况下发生，因此对于没有编译任务的场景，我们不会浪费编译机的吞吐。

    - 批量获取编译配额：对于预取速度不够，并且有多个编译任务排队时，我们会一次性获取多个编译任务的编译机配额，这允许我们均摊请求调度器的延迟，改善性能。

## 处理网络请求

对于网络请求，守护进程目前提供如下能力：

- 向调度器上报本地支持的编译环境（编译器版本等）。
- 接受网络上的编译任务并运行。
